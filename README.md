# Web Crawler e Motor de Busca com Rank Personalizado

Este projeto implementa um mini-motor de busca que:

1. Rastreia (crawl) uma p√°gina inicial, encontra todos os links da p√°gina e, de forma recursiva, segue os links descobertos.

2. Indexa o conte√∫do de cada p√°gina em um banco de dados MySQL, armazenando cada palavra e sua localiza√ß√£o no texto.

Pesquisa links mais relevantes com base em duas (ou mais) palavras de busca, aplicando uma f√≥rmula de rank que considera:

Frequ√™ncia das palavras no texto

Localiza√ß√£o das palavras (quanto mais pr√≥ximo do in√≠cio, melhor)

Dist√¢ncia entre as palavras no texto

O resultado √© uma lista de URLs ordenadas por relev√¢ncia, simulando o funcionamento b√°sico de buscadores como Google, mas em escala reduzida e com pesos ajust√°veis.

‚öôÔ∏è Funcionalidades

Crawler recursivo:
A partir de uma lista de URLs iniciais, encontra e percorre novos links at√© a profundidade definida.

Indexa√ß√£o em banco de dados:
Registra cada palavra (exceto stopwords em portugu√™s) e a posi√ß√£o em que aparece em cada p√°gina.

Ranking de relev√¢ncia:
Combina frequ√™ncia, localiza√ß√£o e dist√¢ncia das palavras usando pesos configur√°veis (w_freq, w_loc, w_dist).

üõ†Ô∏è Tecnologias Utilizadas

Python 3

BeautifulSoup
 ‚Äì parser HTML

NLTK
 ‚Äì tratamento de texto e stopwords em portugu√™s

PyMySQL
 ‚Äì conex√£o com MySQL

urllib3
 ‚Äì requisi√ß√µes HTTP

Banco de dados:

MySQL com tabelas:

urls ‚Äì URLs indexadas

palavras ‚Äì palavras encontradas

palavra_localizacao ‚Äì rela√ß√£o palavra/URL/posi√ß√£o no texto

üìä F√≥rmula de Rank

Cada URL recebe um score calculado como:

score = (w_freq * log(1 + frequ√™ncia))
      + (w_loc  * 1/(1 + localiza√ß√£o))
      + (w_dist * 1/(1 + dist√¢ncia))


frequ√™ncia: n√∫mero de vezes que as palavras aparecem.

localiza√ß√£o: soma das posi√ß√µes das palavras no texto (quanto menor, melhor).

dist√¢ncia: dist√¢ncia entre as palavras (quanto menor, melhor).

Pesos padr√µes:

Frequ√™ncia: 2.0

Localiza√ß√£o: 3.0

Dist√¢ncia: 1.0

Esses valores podem ser ajustados conforme a relev√¢ncia desejada.

üöÄ Como Executar

Configurar o MySQL
Crie um banco de dados chamado indice com as tabelas:

CREATE TABLE urls (
    idurl INT AUTO_INCREMENT PRIMARY KEY,
    url TEXT
);
CREATE TABLE palavras (
    idpalavra INT AUTO_INCREMENT PRIMARY KEY,
    palavra VARCHAR(255)
);
CREATE TABLE palavra_localizacao (
    idurl INT,
    idpalavra INT,
    localizacao INT
);


Configurar o ambiente Python
Instale as depend√™ncias:

pip install beautifulsoup4 urllib3 pymysql nltk lxml


Baixe as stopwords do NLTK:

import nltk
nltk.download('stopwords')


Rodar o crawler
No final do script, defina a URL inicial e a profundidade:

listapaginas = ['https://ge.globo.com/futebol/times/fluminense/']
crawl(listapaginas, 1)


Isso vai indexar a p√°gina inicial e os links encontrados em 1 n√≠vel de profundidade.

Fazer uma pesquisa
Execute:

pesquisa('lesao samuel')


O programa retorna as 10 URLs mais relevantes, com o score calculado.

üìå Exemplo de Sa√≠da
8.5926  https://ge.globo.com/futebol/times/fluminense/noticia/2025/08/29/samuel-xavier-volta-ao-rio-de-janeiro...
4.9537  https://ge.globo.com/futebol/times/fluminense/noticia/2025/08/28/atuacoes-do-fluminense-freytes...
3.7268  https://ge.globo.com/futebol/times/fluminense/noticia/2025/08/29/analise-bola-pune-um-fluminense...
...

üí° Observa√ß√µes

As credenciais de acesso ao banco (usu√°rio e senha) devem ser alteradas para o seu ambiente.

O exemplo usa portugu√™s e stopwords da l√≠ngua portuguesa.

Ideal para estudos de indexa√ß√£o de texto, web crawling e ranking de relev√¢ncia.

üìú Licen√ßa

Este projeto √© distribu√≠do sob a licen√ßa MIT ‚Äì veja o arquivo LICENSE
 para mais detalhes.

Autor

Eduardo Eira ‚Äì LinkedIn
 | GitHub
